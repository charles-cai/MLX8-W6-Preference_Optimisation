# -*- coding: utf-8 -*-
"""Qwen SFT Script (Local Copy)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AmLcHXyr6nrB0fdjkkhR7oTnuYCEu9fC
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import userdata
from huggingface_hub import login

# Grab your saved HF_TOKEN from Colab's secrets manager
hf_token = userdata.get('HF_TOKEN')

# Log into Hugging Face so we can download the model
login(token=hf_token)

print("âœ… Successfully logged into Hugging Face!")

!pip install transformers datasets peft trl bitsandbytes accelerate

!pip install -U trl

!pip install huggingface_hub shutil

# Step 1: Make sure you've got the goods installed
# !pip install transformers datasets peft bitsandbytes accelerate pandas pyarrow

import torch
import shutil  # Importing shutil for file operations
from pathlib import Path  # Importing Path for easier file handling
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model # ðŸ’¡ IMPORT: Added get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,  # ðŸ’¡ IMPORT: Switched to the core Trainer
)
from google.colab import userdata, drive
from huggingface_hub import login

from typing import Dict, List  # ðŸ’¡ FIX: Import List and Dict for type hinting

# --- Hugging Face & Google Drive Login/Mount ---
hf_token = userdata.get('HF_TOKEN')
login(token=hf_token)
print("âœ… Logged into Hugging Face!")

drive.mount('/content/drive')
print("âœ… Google Drive mounted!")


# --- Configuration Section ---
model_name = "Qwen/Qwen3-0.6B-Base"
# Path to the dataset on Google Drive
gdrive_data_path = Path("/content/drive/MyDrive/MLI/mliw6/data/train/data-00000-of-00001.arrow")
# Path to where we'll copy the file in the local Colab environment
local_data_path = Path("/content/data.arrow")


# --- Data Loading and Pre-processing ---
# Copy the file from Google Drive to the local Colab filesystem first.
print(f"Copying data from {gdrive_data_path} to {local_data_path}...")
shutil.copyfile(gdrive_data_path, local_data_path)
print("Copy complete!")

# Load the local file while explicitly disabling the cache to prevent errors.
dataset = load_dataset("arrow", data_files=str(local_data_path), split="train", cache_dir=None)
print("Data loaded successfully with the `datasets` library!")


# --- Model & Tokenizer Loading ---
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# --- Data Formatting and Tokenization ---
def formatting_func(example):
    text = f"### Prompt:\n{example['prompt']}\n\n### Response:\n{example['chosen']}"
    return {"text": text}

formatted_dataset = dataset.map(formatting_func, remove_columns=dataset.column_names)

# ðŸ’¡ NEW: Tokenize the dataset so it's ready for the standard Trainer
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=1024)

tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# --- ðŸ’¡ NEW: Custom Data Collator for Completion-Only Loss ---
# This class manually masks the prompt tokens so the model only learns from the response.
class CompletionOnlyCollator:
    def __init__(self, tokenizer, response_template="### Response:"):
        self.tokenizer = tokenizer
        # The string that separates the prompt from the response
        self.response_template = response_template
        # The tokenized version of the response template
        self.response_template_ids = self.tokenizer.encode(self.response_template, add_special_tokens=False)

    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        # Use the default data collator to handle padding and tensor conversion
        batch = self.tokenizer.pad(features, return_tensors="pt")
        labels = batch["input_ids"].clone()

        # Find the start of the response for each example in the batch
        for i in range(len(features)):
            # Find the index where the response template begins
            response_token_ids_start_idx = -1
            input_ids = features[i]["input_ids"]
            for idx in range(len(input_ids) - len(self.response_template_ids)):
                if input_ids[idx : idx + len(self.response_template_ids)] == self.response_template_ids:
                    response_token_ids_start_idx = idx
                    break

            if response_token_ids_start_idx != -1:
                # Mask all tokens up to and including the response template
                response_token_ids_start_idx += len(self.response_template_ids)
                labels[i, :response_token_ids_start_idx] = -100 # -100 is ignored by the loss function

        batch["labels"] = labels
        return batch


# --- Model Loading ---
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
)
model.config.use_cache = False

# --- LoRA Configuration & Model Wrapping ---
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=[ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj" ],
    bias="none",
    task_type="CAUSAL_LM",
)

# ðŸ’¡ NEW: Manually wrap the model with the LoRA config
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# --- Training Arguments ---
training_args = TrainingArguments(
    output_dir="./results_qwen_base_sft",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    max_steps=10,
    logging_steps=1,
    fp16=True,
    optim="paged_adamw_8bit",
    report_to="none",
)

# --- Initialize the Trainer ---
# ðŸ’¡ CHANGE: Using the standard `Trainer` now
trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

# --- Start Training ---
print("ðŸš€ Starting SFT training with the standard `transformers.Trainer`...")
trainer.train()
print("âœ… Base model training complete. We did it.")

from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# --- Configuration ---
base_model_name = "Qwen/Qwen3-0.6B-Base"
# This path should point to where your trained LoRA adapter was saved.
# The Trainer saves checkpoints in this folder.
adapter_path = "./results_qwen_base_sft/checkpoint-10"

# --- Load the Base Model and Tokenizer ---
# For inference, it's often best to load in full precision (not 4-bit)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.pad_token = tokenizer.eos_token # Make sure padding token is set

# --- Load the LoRA Adapter ---
# PEFT will automatically merge the adapter weights into the base model
print("ðŸš€ Loading PEFT adapter...")
model = PeftModel.from_pretrained(base_model, adapter_path)
model = model.merge_and_unload() # Merge weights for faster inference
print("âœ… Adapter loaded and merged!")

# --- Prepare the Inference Prompt ---
# Use the exact same format as your training data!
new_prompt_text = "My Girlfriend of 15 months went through my Facebook messages without my permission and found old conversations of me flirting with a girl. She broke up with me and went no contact."
formatted_prompt = f"### Prompt:\n{new_prompt_text}\n\n### Response:\n"

# --- Generate the Prediction ---
# Tokenize the input prompt
inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")

print("\nðŸ¤– Generating summary...")
# Use the model's generate function
outputs = model.generate(
    **inputs,
    max_new_tokens=50,  # Limit the length of the summary
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True,     # Use sampling for more creative outputs
    temperature=0.7,
    top_p=0.9,
)

# Decode the generated tokens to text
predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\nâœ¨ Predicted Summary:")
print(predicted_text)